{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATE_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZqsRKHX1k6Z",
        "colab_type": "text"
      },
      "source": [
        "#Singe Domain Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JZbioMx1aSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we first connect to my google drive in order to collect all the data\n",
        "#we will have three domains (restaurant, hotels and electronics)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgNi7LOt1p-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "#load all the datasets\n",
        "data_lap = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/laptops_2014.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_lap = data_lap.fillna(method=\"ffill\")\n",
        "\n",
        "data_comp = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Computer.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_comp = data_comp.fillna(method=\"ffill\")\n",
        "\n",
        "data_router = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Router.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_router = data_router.fillna(method=\"ffill\")\n",
        "\n",
        "data_speaker = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Speaker.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_speaker = data_speaker.fillna(method=\"ffill\")\n",
        "\n",
        "data_rest = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/restaurants1.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_rest = data_rest.fillna(method=\"ffill\")\n",
        "\n",
        "data_hotels = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/hotels.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_hotels = data_hotels.fillna(method=\"ffill\")\n",
        "\n",
        "#We also concatenate all the datasets\n",
        "data = pd.concat([data_comp,data_router,data_speaker,data_rest,data_hotels], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqVbj9Yb2n7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Class for handling the dataset\n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w,t) for w,t in zip(s[\"TOKEN\"].values.tolist(), s[\"TAG\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"SENTENCE\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyeGVxRz2368",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getter_lap = SentenceGetter(data_lap)\n",
        "getter_comp = SentenceGetter(data_comp)\n",
        "getter_router = SentenceGetter(data_router)\n",
        "getter_speaker = SentenceGetter(data_speaker)\n",
        "getter_rest = SentenceGetter(data_rest)\n",
        "getter_hotels = SentenceGetter(data_hotels)\n",
        "\n",
        "#We get all the sentences\n",
        "#Each sentence is a string (not splitted into tokens)\n",
        "sentences_lap = [\" \".join([str(s[0]) for s in sent]) for sent in getter_lap.sentences]\n",
        "sentences_comp = [\" \".join([str(s[0]) for s in sent]) for sent in getter_comp.sentences]\n",
        "sentences_router = [\" \".join([str(s[0]) for s in sent]) for sent in getter_router.sentences]\n",
        "sentences_speaker = [\" \".join([str(s[0]) for s in sent]) for sent in getter_speaker.sentences]\n",
        "sentences_rest = [\" \".join([str(s[0]) for s in sent]) for sent in getter_rest.sentences]\n",
        "sentences_hotels = [\" \".join([str(s[0]) for s in sent]) for sent in getter_hotels.sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zqrDuxg3UC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We print a sentence for each dataset\n",
        "print(sentences_lap[0])\n",
        "print(sentences_comp[0])\n",
        "print(sentences_speaker[0])\n",
        "print(sentences_router[0])\n",
        "print(sentences_rest[0])\n",
        "print(sentences_hotels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh5rBlHO3YIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_lap = [[s[1] for s in sent] for sent in getter_lap.sentences]\n",
        "labels_comp = [[s[1] for s in sent] for sent in getter_comp.sentences]\n",
        "labels_router = [[s[1] for s in sent] for sent in getter_router.sentences]\n",
        "labels_speaker = [[s[1] for s in sent] for sent in getter_speaker.sentences]\n",
        "labels_rest = [[s[1] for s in sent] for sent in getter_rest.sentences]\n",
        "labels_hotels = [[s[1] for s in sent] for sent in getter_hotels.sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRv-i0Dl3dol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sentences_lap[0])\n",
        "print(labels_lap[0])\n",
        "print(sentences_comp[0])\n",
        "print(labels_comp[0])\n",
        "print(sentences_speaker[0])\n",
        "print(labels_speaker[0])\n",
        "print(sentences_router[0])\n",
        "print(labels_router[0])\n",
        "print(sentences_rest[0])\n",
        "print(labels_rest[0])\n",
        "print(sentences_hotels[0])\n",
        "print(labels_hotels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLulCvSX3i8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We get a dictionary for mapping tags do ids\n",
        "tags_vals = list(set(data[\"TAG\"].values))\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
        "tag2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfAvRiSO3neA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert==0.4.0\n",
        "import torch\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm4UH_m33oyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We check if the gpu is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "n_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRzAblMy30uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We tokenize all the sentences\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenized_texts_lap = [tokenizer.tokenize(sent) for sent in sentences_lap]\n",
        "tokenized_texts_comp = [tokenizer.tokenize(sent) for sent in sentences_comp]\n",
        "tokenized_texts_speaker = [tokenizer.tokenize(sent) for sent in sentences_speaker]\n",
        "tokenized_texts_router = [tokenizer.tokenize(sent) for sent in sentences_router]\n",
        "tokenized_texts_rest = [tokenizer.tokenize(sent) for sent in sentences_rest]\n",
        "tokenized_texts_hotels = [tokenizer.tokenize(sent) for sent in sentences_hotels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b31eYy83-wX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 75\n",
        "bs = 32\n",
        "\n",
        "input_ids_lap = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_lap],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_ids_comp = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_comp],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_ids_speaker = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_speaker],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_ids_router = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_router],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_ids_rest = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_rest],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_ids_hotels = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_hotels],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcsND6AA4XuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags_lap = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_lap],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "tags_comp = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_comp],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "tags_speaker = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_speaker],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "tags_router = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_router],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "tags_rest = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_rest],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "tags_hotels = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_hotels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb3nPdwt5nxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_masks_lap = [[float(i>0) for i in ii] for ii in input_ids_lap]\n",
        "attention_masks_comp = [[float(i>0) for i in ii] for ii in input_ids_comp]\n",
        "attention_masks_speaker = [[float(i>0) for i in ii] for ii in input_ids_speaker]\n",
        "attention_masks_router = [[float(i>0) for i in ii] for ii in input_ids_router]\n",
        "attention_masks_rest = [[float(i>0) for i in ii] for ii in input_ids_rest]\n",
        "attention_masks_hotels = [[float(i>0) for i in ii] for ii in input_ids_hotels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkelot8y5prP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install seqeval\n",
        "from sklearn.model_selection import KFold\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "cv = KFold(n_splits=5, random_state=42, shuffle=False)\n",
        "i = 0\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "for train_index, test_index in cv.split(attention_masks_comp):\n",
        "  training_inputs = input_ids_comp[train_index]\n",
        "  test_inputs = input_ids_comp[test_index]\n",
        "  training_tags = tags_comp[train_index]\n",
        "  test_tags = tags_comp[test_index]\n",
        "  training_masks = np.array(attention_masks_comp)[train_index]\n",
        "  test_masks = np.array(attention_masks_comp)[test_index]\n",
        "  validation_inputs = training_inputs[-(int(0.2*len(training_inputs))):]\n",
        "  validation_tags = training_tags[-(int(0.2*len(training_inputs))):]\n",
        "  validation_masks = training_masks[-(int(0.2*len(training_inputs))):]\n",
        "  training_inputs = training_inputs[:(int(0.8*len(training_inputs)))]\n",
        "  training_tags = training_tags[:(int(0.8*len(training_tags)))]\n",
        "  training_masks = training_masks[:(int(0.8*len(training_masks)))]\n",
        "\n",
        "  tr_inputs = torch.tensor(training_inputs)\n",
        "  val_inputs = torch.tensor(validation_inputs)\n",
        "  te_inputs = torch.tensor(test_inputs)\n",
        "  tr_tags = torch.tensor(training_tags)\n",
        "  val_tags = torch.tensor(validation_tags)\n",
        "  te_tags = torch.tensor(test_tags)\n",
        "  tr_masks = torch.tensor(training_masks)\n",
        "  val_masks = torch.tensor(validation_masks)\n",
        "  te_masks = torch.tensor(test_masks)\n",
        "  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "  valid_sampler = SequentialSampler(valid_data)\n",
        "  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
        "\n",
        "  test_data = TensorDataset(te_inputs, te_masks, te_tags)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n",
        "  model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
        "  model.cuda();\n",
        "  FULL_FINETUNING = True\n",
        "  if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.01},\n",
        "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.0}]\n",
        "  else:\n",
        "    param_optimizer = list(model.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "  optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
        "  epochs = 10\n",
        "  max_grad_norm = 1.0\n",
        "\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    # VALIDATION on validation set\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask, labels=b_labels)\n",
        "            logits = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        \n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
        "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
        "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      tmp_eval_loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    true_labels.append(label_ids)\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    nb_eval_examples += b_input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
        "  valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
        "  print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "  print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  print(classification_report(valid_tags,pred_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXrA2V4Z-SQy",
        "colab_type": "text"
      },
      "source": [
        "# Cross Domain Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aewVBlue7ZWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We generate training and test ids, masks and tags\n",
        "tr_inputs_lap, val_inputs_lap, tr_tags_lap, val_tags_lap = train_test_split(input_ids_lap, tags_lap, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "tr_masks_lap, val_masks_lap, _, _ = train_test_split(attention_masks_lap, input_ids_lap,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n",
        "tr_inputs_comp, val_inputs_comp, tr_tags_comp, val_tags_comp = train_test_split(input_ids_comp, tags_comp, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "tr_masks_comp, val_masks_comp, _, _ = train_test_split(attention_masks_comp, input_ids_comp,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n",
        "tr_inputs_speaker, val_inputs_speaker, tr_tags_speaker, val_tags_speaker = train_test_split(input_ids_speaker, tags_speaker, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "tr_masks_speaker, val_masks_speaker, _, _ = train_test_split(attention_masks_speaker, input_ids_speaker,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n",
        "tr_inputs_router, val_inputs_router, tr_tags_router, val_tags_router = train_test_split(input_ids_router, tags_router, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "tr_masks_router, val_masks_router, _, _ = train_test_split(attention_masks_router, input_ids_router,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n",
        "tr_inputs_rest, val_inputs_rest, tr_tags_rest, val_tags_rest = train_test_split(input_ids_rest, tags_rest, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "tr_masks_rest, val_masks_rest, _, _ = train_test_split(attention_masks_rest, input_ids_rest,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n",
        "tr_inputs_hotels, val_inputs_hotels, tr_tags_hotels, val_tags_hotels = train_test_split(input_ids_hotels, tags_hotels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "tr_masks_hotels, val_masks_hotels, _, _ = train_test_split(attention_masks_hotels, input_ids_hotels,\n",
        "                                             random_state=2018, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJNha1bS-eZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We concatenate the ids of five over six datasets\n",
        "training_inputs = np.concatenate((tr_inputs_speaker,tr_inputs_comp, tr_inputs_router, tr_inputs_rest, tr_inputs_lap))\n",
        "validation_inputs = np.concatenate((val_inputs_speaker,val_inputs_comp, val_inputs_router, val_inputs_rest, val_inputs_lap))\n",
        "training_tags = np.concatenate((tr_tags_speaker,tr_tags_comp, tr_tags_router, tr_tags_rest, tr_tags_lap))\n",
        "validation_tags = np.concatenate((val_tags_speaker,val_tags_comp,val_tags_router, val_tags_rest, val_tags_lap))\n",
        "training_masks = np.concatenate((tr_masks_speaker,tr_masks_comp, tr_masks_router, tr_masks_rest, tr_masks_lap))\n",
        "validation_masks = np.concatenate((val_masks_speaker,val_masks_comp, val_masks_router, val_masks_rest, val_masks_lap))\n",
        "test_inputs = np.concatenate((tr_inputs_hotels,val_inputs_hotels))\n",
        "test_tags = np.concatenate((tr_tags_hotels,val_tags_hotels))\n",
        "test_masks = np.concatenate((tr_masks_hotels,val_masks_hotels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tw-ZT6i-gkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We generate the torch tensors to feed the model\n",
        "tr_inputs = torch.tensor(training_inputs)\n",
        "val_inputs = torch.tensor(validation_inputs)\n",
        "te_inputs = torch.tensor(test_inputs)\n",
        "tr_tags = torch.tensor(training_tags)\n",
        "val_tags = torch.tensor(validation_tags)\n",
        "te_tags = torch.tensor(test_tags)\n",
        "tr_masks = torch.tensor(training_masks)\n",
        "val_masks = torch.tensor(validation_masks)\n",
        "te_masks = torch.tensor(test_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDOnUvzF-kQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We generate the DataLoader\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
        "\n",
        "test_data = TensorDataset(te_inputs, te_masks, te_tags)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub4vGmsz-yei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We perform five iterations on the cross domain experiments\n",
        "!pip install seqeval\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "#function to extract the aspects from the sentence \n",
        "def print_f1_on_file(filename,true_aspects,extracted_aspects):\n",
        "  count = 0\n",
        "  f1 = 0.0\n",
        "  f = open(filename,\"w+\")\n",
        "  for ea,ta in zip(extracted_aspects,true_aspects):\n",
        "    if 'B-aspect' in ta:\n",
        "      f.write(str(f1_score([ta],[ea]))+\"\\n\")\n",
        "      count += 1\n",
        "      f1 += f1_score([ta],[ea])\n",
        "  f.close()\n",
        "  print(f1)\n",
        "  print(count)\n",
        "  print(f1/count)\n",
        "  \n",
        "for i in range(5):\n",
        "  model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
        "  model.cuda();\n",
        "  FULL_FINETUNING = True\n",
        "  if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.01},\n",
        "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.0}]\n",
        "  else:\n",
        "    param_optimizer = list(model.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "  optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
        "  epochs = 10\n",
        "  max_grad_norm = 1.0\n",
        "\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    # VALIDATION on validation set\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask, labels=b_labels)\n",
        "            logits = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        \n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
        "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
        "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      tmp_eval_loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    true_labels.append(label_ids)\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    nb_eval_examples += b_input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
        "  valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
        "  print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "  print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "  print(classification_report(valid_tags,pred_tags))\n",
        "  print_f1_on_file(\"/content/drive/My Drive/Tesi_ABSA/hotels_sampled_bert_f1_\"+str(i+1)+\".txt\",valid_tags,pred_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iikW0Tsv-4g5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "idx2tag = {i: w for w, i in tags2index.items()}\n",
        "print(idx2tag)\n",
        "#Function to convert the predictions to tags\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "\n",
        "max_len = 100\n",
        "\n",
        "#We extract the aspects of 32.000 sentences at each iteration in order to avoid memory problems\n",
        "start = 0\n",
        "while start < 2112337:\n",
        "  f_rev = open(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/review_divided.txt\")\n",
        "  reviews = []\n",
        "  id_review = []\n",
        "  id_sentence = []\n",
        "  user_ids = []\n",
        "  business_ids = []\n",
        "  comp_list = []\n",
        "  neg_list = []\n",
        "  neu_list = []\n",
        "  pos_list = []\n",
        "  count = 0\n",
        "  #We skip all the sentences already processed\n",
        "  for line in f_rev:\n",
        "    if count < start:\n",
        "      count +=1\n",
        "      continue\n",
        "    splitted = line.split(\"\\t\")\n",
        "    compund = float(splitted[2])\n",
        "    neg = float(splitted[3])\n",
        "    neu = float(splitted[4])\n",
        "    pos = float(splitted[5])\n",
        "    sentence = splitted[8]\n",
        "    num_review = int(splitted[0])\n",
        "    num_sentence = int(splitted[1])\n",
        "    user_id = splitted[6]\n",
        "    business_id = splitted[7]\n",
        "    reviews.append(sentence)\n",
        "    id_review.append(num_review)\n",
        "    id_sentence.append(num_sentence)\n",
        "    user_ids.append(user_id)\n",
        "    business_ids.append(business_id)\n",
        "    comp_list.append(compund)\n",
        "    neg_list.append(neg)\n",
        "    neu_list.append(neu)\n",
        "    pos_list.append(pos)\n",
        "    count +=1\n",
        "    #If we have loaded 32.000 sentences, we stop the loop\n",
        "    if count == start+32000:\n",
        "      break\n",
        "\n",
        "  #We extract all the aspects in the sentences with the trained model\n",
        "  new_X_rest = []\n",
        "  rev_splitted = []\n",
        "  for rev in reviews:\n",
        "    rev_splitted.append(rev.split())\n",
        "  for seq in rev_splitted:\n",
        "      new_seq = []\n",
        "      for i in range(max_len):\n",
        "        try:\n",
        "          new_seq.append(seq[i])\n",
        "        except:\n",
        "          new_seq.append(\"PADword\")\n",
        "      new_X_rest.append(new_seq)\n",
        "  for i in range(16):\n",
        "    new_seq=[]\n",
        "    for j in range(max_len):\n",
        "      new_seq.append(\"PADword\")\n",
        "    new_X_rest.append(new_seq)\n",
        "  print(int(len(reviews)/32)*32)\n",
        "  \n",
        "\n",
        "  print(len(new_X_rest))\n",
        "  test_pred = model.predict(np.array(new_X_rest[:(int(len(new_X_rest)/32)*32)]), verbose=1)\n",
        "  pred_labels = pred2label(test_pred)\n",
        "  aspect_words = []\n",
        "  begin_end_indeces = []\n",
        "  f = open(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/extracted_aspect_terms_wo_restaurants.txt\",\"a+\")\n",
        "  f_ind = open(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/extracted_aspect_index_wo_restaurants.txt\",\"a+\")\n",
        "  #We convert the tags into aspect terms\n",
        "  for (tokens, labels,rev_n,sent_n,user_id,business_id) in zip(new_X_rest, pred_labels,id_review,id_sentence,user_ids,business_ids):\n",
        "      f.write(str(rev_n)+\"\\t\")\n",
        "      f.write(str(sent_n)+\"\\t\")\n",
        "      f.write(user_id+\"\\t\")\n",
        "      f.write(business_id+\"\\t\")\n",
        "      sentence_aspects = []\n",
        "      begin_indices = [i for i, x in enumerate(labels) if x == \"B-aspect\"]\n",
        "      for bi in begin_indices:\n",
        "        current_aspect = tokens[bi]\n",
        "        count = 1\n",
        "        while (count < len(labels)-bi and labels[bi+count] == 'I-aspect'):\n",
        "          current_aspect += ' '+tokens[bi+count]\n",
        "          count += 1\n",
        "        begin_end_indeces.append([bi,(bi+count-1)])\n",
        "        f.write(current_aspect+\"\\t\")\n",
        "      f_ind.write(str(rev_n)+\"\\t\"+str(sent_n)+\"\\t\"+str(begin_end_indeces)+\"\\n\")\n",
        "      begin_end_indeces = []\n",
        "      f.write(\"\\n\")\n",
        "  f_ind.close()\n",
        "  f.close()\n",
        "  start += 32000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUkkX-YlH5qJ",
        "colab_type": "text"
      },
      "source": [
        "# Aspect clustering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9-PKTCtHsE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We save all the aspects into a list\n",
        "aspects = []\n",
        "f = open(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/extracted_aspect_terms_wo_restaurants.txt\")\n",
        "aspects = []\n",
        "ids_reviews = []\n",
        "ids_sentences = []\n",
        "for line in f.readlines():\n",
        "\n",
        "  splitted_line = line.split(\"\\t\")\n",
        "  ids_reviews.append(splitted_line[0])\n",
        "  ids_sentences.append(splitted_line[1])\n",
        "  aspects.append(splitted_line[4:])\n",
        "  \n",
        "print(len(aspects))\n",
        "full_aspects_list = []\n",
        "for asp in aspects:\n",
        "  for el in asp:\n",
        "    if el != \"\\n\":\n",
        "      full_aspects_list.append(el.lower())\n",
        "\n",
        "print(full_aspects_list[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQmUwICQHx_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading all the indeces from the file \n",
        "f_ind = open(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/extracted_aspect_index_wo_restaurants.txt\")\n",
        "positions = []\n",
        "for line in f_ind:\n",
        "  positions.append(line.split(\"\\t\")[2])\n",
        "print(positions[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1rZnHdTH2dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We download the pre-trained word2vec model\n",
        "import gensim\n",
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E3ZO7lPIB4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We generate the embedding matrix\n",
        "import numpy as np\n",
        "filepath = \"GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "embeddings = {}\n",
        "from gensim.models import KeyedVectors\n",
        "print(\"Loading the Word2Vec model...\")\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(filepath, binary=True) \n",
        "for word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n",
        "    coefs = np.asarray(vector, dtype='float32')\n",
        "    embeddings[word] = coefs\n",
        "print('# vectors:',  len(embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZuSQip1IGKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "#We get the vectorial representation of each aspect term\n",
        "aspects_embeddings = []\n",
        "final_aspects = []\n",
        "print(len(aspects))\n",
        "for term in full_aspects_list:\n",
        "  current_vector = [0]*300\n",
        "  splitted_term = term.split()\n",
        "  count = 0\n",
        "  for t in splitted_term:\n",
        "    try:\n",
        "      current_vector += embeddings[t]\n",
        "      count += 1\n",
        "    except:\n",
        "      print('I got a KeyError - reason: ',t)\n",
        "      continue\n",
        "  try:\n",
        "    current_vector[:] = [x / count for x in current_vector]\n",
        "  except:\n",
        "    current_vector = current_vector\n",
        "  aspects_embeddings.append(current_vector)\n",
        "  final_aspects.append(term)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V69UcZ2cIJvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We use the k-means on the vector representations of aspect terms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=50, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "y_pred = kmeans.fit_predict(aspects_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUr9zf0iIQ9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We print on the same file the aspect terms belonging to the same cluster\n",
        "\n",
        "for aspect,cluster in zip(full_aspects_list,y_pred):\n",
        "  f = open(\"aspect_cluster_k50_wo_restaurants_\"+str(cluster)+\".txt\",\"a+\")\n",
        "  f.write(aspect+\"\\n\")\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOb-jTB-Ih39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We generate a final json containing all the information extracted during this process\n",
        "import json\n",
        "json_list = []\n",
        "count = 0\n",
        "f_rev = open(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/review_divided.txt\")\n",
        "reviews = []\n",
        "id_review = []\n",
        "id_sentence = []\n",
        "user_ids = []\n",
        "business_ids = []\n",
        "comp_list = []\n",
        "neg_list = []\n",
        "neu_list = []\n",
        "pos_list = []\n",
        "for line in f_rev:\n",
        "    splitted = line.split(\"\\t\")\n",
        "    compund = float(splitted[2])\n",
        "    neg = float(splitted[3])\n",
        "    neu = float(splitted[4])\n",
        "    pos = float(splitted[5])\n",
        "    num_review = int(splitted[0])\n",
        "    num_sentence = int(splitted[1])\n",
        "    user_id = splitted[6]\n",
        "    business_id = splitted[7]\n",
        "    id_review.append(num_review)\n",
        "    id_sentence.append(num_sentence)\n",
        "    user_ids.append(user_id)\n",
        "    business_ids.append(business_id)\n",
        "    comp_list.append(compund)\n",
        "    neg_list.append(neg)\n",
        "    neu_list.append(neu)\n",
        "    pos_list.append(pos)\n",
        "\n",
        "print(len(comp_list))\n",
        "for rev_id, sent_id, asp, pos,compound,negative,neutral,positive,user,business in zip (id_review,id_sentence,aspects,positions,comp_list,neg_list,neu_list,pos_list,user_ids,business_ids):\n",
        "  idx = json.loads(pos[:-1])\n",
        "  aspects_list = []\n",
        "  if len(asp) != 0:\n",
        "    for id,aspect in zip(idx,asp):\n",
        "      a = {\n",
        "          \"aspect_term\":aspect,\n",
        "          \"id_starting_token\":id[0],\n",
        "          \"id_ending_token\":id[1],\n",
        "          \"cluster\":str(y_pred[count]),\n",
        "          \"compund\":str(compound),\n",
        "          \"negative\":str(negative),\n",
        "          \"neutral\":str(neutral),\n",
        "          \"positive\":str(positive)\n",
        "      }\n",
        "      aspects_list.append((a))\n",
        "      count+=1\n",
        "    x = {\n",
        "        \"id_reviews\":rev_id,\n",
        "        \"id_sentence\":sent_id,\n",
        "        \"aspects\":aspects_list\n",
        "    }\n",
        "    json_list.append(x)\n",
        "\n",
        "final_json = json.dumps(json_list,indent=2)\n",
        "f = open(\"json_extracted_aspects_k50_wo_restaurants.json\",\"w+\")\n",
        "f.write(final_json)\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}