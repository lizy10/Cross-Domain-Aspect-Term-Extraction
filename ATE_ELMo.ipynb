{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATE_ELMo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "970lXOfxU1T2",
        "colab_type": "text"
      },
      "source": [
        "# Single Domain Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhOzLLLcVPNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f5ba62de-a286-448d-ec1e-bdd03f30aae9"
      },
      "source": [
        "#we first connect to my google drive in order to collect all the data\n",
        "#we will have three domains (restaurant, hotels and electronics)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEW3NvW-UyMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to extract the aspects from the sentence and calculate the f1 for each sentence\n",
        "#The f1 values will be printed on a file in order to compute statistics\n",
        "def print_f1_on_file(filename,true_aspects,extracted_aspects):\n",
        "  count = 0\n",
        "  f1 = 0.0\n",
        "  f = open(filename,\"w+\")\n",
        "  for ea,ta in zip(extracted_aspects,true_aspects):\n",
        "    if 'B-aspect' in ta:\n",
        "      f.write(str(f1_score([ta],[ea]))+\"\\n\")\n",
        "      count += 1\n",
        "      f1 += f1_score([ta],[ea])\n",
        "  f.close()\n",
        "  print(f1)\n",
        "  print(count)\n",
        "  print(f1/count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o-LLlJ2VLbA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb4a2869-84f4-4519-f220-bff19d49e9e8"
      },
      "source": [
        "#we use the tensorflow version 1.x\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnMiasE0Velj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "34a08e09-6cc2-4ffa-bd72-71a310c5fa4f"
      },
      "source": [
        "#first try with restaurant dataset\n",
        "#In this case we perform a single domain experiment on the restaurant dataset\n",
        "#If we are interested in other domains, we only need to change the file\n",
        "data = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/restaurants1.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data = data.fillna(method=\"ffill\")\n",
        "data.tail(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SENTENCE</th>\n",
              "      <th>TOKEN</th>\n",
              "      <th>TAG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71615</th>\n",
              "      <td>4673</td>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71616</th>\n",
              "      <td>4673</td>\n",
              "      <td>various</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71617</th>\n",
              "      <td>4673</td>\n",
              "      <td>vegetables</td>\n",
              "      <td>B-aspect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71618</th>\n",
              "      <td>4673</td>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71619</th>\n",
              "      <td>4673</td>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71620</th>\n",
              "      <td>4673</td>\n",
              "      <td>rice</td>\n",
              "      <td>B-aspect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71621</th>\n",
              "      <td>4673</td>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71622</th>\n",
              "      <td>4673</td>\n",
              "      <td>glass</td>\n",
              "      <td>B-aspect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71623</th>\n",
              "      <td>4673</td>\n",
              "      <td>noodles</td>\n",
              "      <td>I-aspect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71624</th>\n",
              "      <td>4673</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       SENTENCE       TOKEN       TAG\n",
              "71615      4673           ,         O\n",
              "71616      4673     various         O\n",
              "71617      4673  vegetables  B-aspect\n",
              "71618      4673           ,         O\n",
              "71619      4673         and         O\n",
              "71620      4673        rice  B-aspect\n",
              "71621      4673         and         O\n",
              "71622      4673       glass  B-aspect\n",
              "71623      4673     noodles  I-aspect\n",
              "71624      4673           .         O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Ejs1LYVvZn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cfa12bf-68ad-46bd-fe57-a4f258d9dee3"
      },
      "source": [
        "#creation of a set containing the words\n",
        "#this will be useful in order to create a mappin between the tokens in the sentences and the words\n",
        "words = set(list(data['TOKEN'].values))\n",
        "words.add('PADword')\n",
        "n_words = len(words)\n",
        "print(n_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tccMEN77V28D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b47b3d1a-f0ad-4576-ec90-86196e55ce0c"
      },
      "source": [
        "#Creation of a set containing all the possible tags\n",
        "tags = list(set(data[\"TAG\"].values))\n",
        "n_tags = len(tags)\n",
        "print(n_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qZRafaoV91D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#class for get sentences in the format of tuples [(TOKEN,TAG),(TOKEN,TAG)...]\n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"TOKEN\"].values.tolist(),s[\"TAG\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"SENTENCE\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkIHO__gWAhL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fc3f1e3d-123d-4628-deb5-e688155ef292"
      },
      "source": [
        "#we get all the sentences and we print the longest sentence\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences\n",
        "print(\"# of sentences: \",len(sentences))\n",
        "largest_sen = max(len(sen) for sen in sentences)\n",
        "print('Biggest sentence has {} words'.format(largest_sen))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of sentences:  4674\n",
            "Biggest sentence has 76 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHcOm1M9WD8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "10b423ba-f55a-42f7-8033-35bb14855784"
      },
      "source": [
        "#we print the first sentence in the above format\n",
        "print(sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Judging', 'O'), ('from', 'O'), ('previous', 'O'), ('posts', 'O'), ('this', 'O'), ('used', 'O'), ('to', 'O'), ('be', 'O'), ('a', 'O'), ('good', 'O'), ('place', 'B-aspect'), (',', 'O'), ('but', 'O'), ('not', 'O'), ('any', 'O'), ('longer', 'O'), ('.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04CNuI3VWMJl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8edb9a9a-e94a-4371-e137-60a6975f6f2e"
      },
      "source": [
        "#we create the X dataset containing only the padded sentences' tokens\n",
        "max_len = 75\n",
        "X = [[w[0]for w in s] for s in sentences]\n",
        "new_X = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X.append(new_seq)\n",
        "\n",
        "print(new_X[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', ',', 'but', 'not', 'any', 'longer', '.', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qY1862mWSDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e5fdab2-cfd1-45a2-84d1-3196dd7bd366"
      },
      "source": [
        "#we convert the tags into indeces\n",
        "#we also create the padded y\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tags2index = {t:i for i,t in enumerate(tags)}\n",
        "y = [[tags2index[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tags2index[\"O\"])\n",
        "print(\"Mapping between indeces and tags:\",tags2index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mapping between indeces and tags: {'I-aspect': 0, 'B-aspect': 1, 'O': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mgOVqI4WvgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "a3be09cc-5be1-46c3-b4e0-36c608eed0f3"
      },
      "source": [
        "#We instal seqeval for evaluating the perfomances of the model\n",
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /tensorflow-1.15.2/python3.6 (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=dde500a0963ae1458b7b76922dd920dec08b618d46ea09423a97fe370d37e3f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvUXn-InWuhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We import the needed libraries\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from keras import backend as K\n",
        "from keras.models import Model, Input\n",
        "from keras.layers.merge import add\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsPDKzU9Wwai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We define the ELMo embedding function that will be used by our embedding layer in the neural network\n",
        "batch_size = 32\n",
        "def ElmoEmbedding(x):\n",
        "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x,    tf.string)),\"sequence_len\": tf.constant(batch_size*[max_len])\n",
        "                     },\n",
        "                      signature=\"tokens\",\n",
        "                      as_dict=True)[\"elmo\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1vas3NwXnsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to convert the predictions into tags using the argmax function\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "\n",
        "#Function to convert the test indeces into tags   \n",
        "def test2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            out_i.append(idx2tag[p].replace(\"PADword\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAnFt9ejWljZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "ac5ad076-d06c-4c6b-d85f-c0128726a15d"
      },
      "source": [
        "#We create a dictionary for mapping indeces into tags\n",
        "idx2tag = {i: w for w, i in tags2index.items()}\n",
        "print(idx2tag)\n",
        "\n",
        "cv = KFold(n_splits=5, random_state=42, shuffle=False)\n",
        "i = 0\n",
        "\n",
        "#We iterate over the folds\n",
        "for train_index, test_index in cv.split(new_X):\n",
        "    x_pd = pd.Series(new_X)\n",
        "    y_pd = y\n",
        "\n",
        "    #We generate the current training set and test set\n",
        "    X_tr, X_te, y_tr, y_te = list(x_pd[train_index]),list(x_pd[test_index]),(y_pd[train_index]),(y_pd[test_index])\n",
        "    \n",
        "    #We download the pretrained elmo model\n",
        "    sess = tf.Session()\n",
        "    K.set_session(sess)\n",
        "    elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "\n",
        "    #We define our model\n",
        "    input_text = Input(shape=(max_len,), dtype=tf.string)\n",
        "    embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\n",
        "    \n",
        "    x = Bidirectional(LSTM(units=256, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
        "    x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "    out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
        "    model = Model(input_text, out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    l = len(X_tr)\n",
        "\n",
        "    #We also divide the training set into training and validation set\n",
        "    X_tr, X_val = X_tr[:int(l*0.8)], X_tr[-(int(l*0.2)):]\n",
        "    y_tr, y_val = y_tr[:int(l*0.8)], y_tr[-(int(l*0.2)):]\n",
        "    \n",
        "\n",
        "    #We convert training, validation and test size into a multiple of the batch size\n",
        "    #training set\n",
        "    l = len(X_tr)\n",
        "    for i in range(0, 32 - (l%32)):\n",
        "      new_seq = []\n",
        "      new_tags = []\n",
        "      for j in range(max_len):\n",
        "        new_seq.append(\"PADword\")\n",
        "        new_tags.append(tags2index['O'])\n",
        "      y_tr = np.append(y_tr,np.array(new_tags).reshape((1,75)),axis=0)\n",
        "      X_tr.append(new_seq)\n",
        "\n",
        "    #validation set\n",
        "    l = len(X_val)\n",
        "    for i in range(0, 32 - (l%32)):\n",
        "      new_seq = []\n",
        "      new_tags = []\n",
        "      for j in range(max_len):\n",
        "        new_seq.append(\"PADword\")\n",
        "        new_tags.append(tags2index['O'])\n",
        "      y_val = np.append(y_val,np.array(new_tags).reshape((1,75)),axis = 0)\n",
        "      X_val.append(new_seq)\n",
        "\n",
        "    #test set\n",
        "    l = len(X_te)\n",
        "    for i in range(0, 32 - (l%32)):\n",
        "      new_seq = []\n",
        "      for j in range(max_len):\n",
        "        new_seq.append(\"PADword\")\n",
        "      X_te.append(new_seq)\n",
        "\n",
        "    y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
        "    y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n",
        "\n",
        "    #fitting the model\n",
        "    history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),batch_size=batch_size, epochs=10, verbose=1)\n",
        "\n",
        "    #estimating the predictions\n",
        "    test_pred = model.predict(np.array(X_te), verbose=1)\n",
        "    pred_labels = pred2label(test_pred)\n",
        "    test_labels = test2label(y_te)\n",
        "\n",
        "    #print the current classification report\n",
        "    print(classification_report(test_labels,pred_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'I-aspect', 1: 'B-aspect', 2: 'O'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3008 samples, validate on 768 samples\n",
            "Epoch 1/10\n",
            "2528/3008 [========================>.....] - ETA: 3:32 - loss: 0.0728 - accuracy: 0.9720"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c3124a546d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m#fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m#estimating the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pifZg2PkefnN",
        "colab_type": "text"
      },
      "source": [
        "# Cross Domain Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVLom8NvYthB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we load all the datasets \n",
        "data_lap = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/laptops_2014.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_lap = data_lap.fillna(method=\"ffill\")\n",
        "\n",
        "data_comp = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Computer.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_comp = data_comp.fillna(method=\"ffill\")\n",
        "\n",
        "data_router = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Router.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_router = data_router.fillna(method=\"ffill\")\n",
        "\n",
        "data_speaker = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Speaker.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_speaker = data_speaker.fillna(method=\"ffill\")\n",
        "\n",
        "data_rest = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/restaurants1.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_rest = data_rest.fillna(method=\"ffill\")\n",
        "\n",
        "data_hotels = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/hotels.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_hotels = data_hotels.fillna(method=\"ffill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhuC6DD1elrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c5c86e4-444d-4a2b-dea6-ba647dbdbf87"
      },
      "source": [
        "#creation of a set containing the words\n",
        "#this will be useful in order to create a mappin between the tokens in the sentences and the words\n",
        "#it is the first step for the creation of a numerical representation of the sentences\n",
        "#we also add the word PADword for feed the sentences with less than 75 words in order to have all sentences with length 75\n",
        "words_lap = set(list(data_lap['TOKEN'].values))\n",
        "words_comp = set(list(data_comp['TOKEN'].values))\n",
        "words_router = set(list(data_router['TOKEN'].values))\n",
        "words_speaker = set(list(data_speaker['TOKEN'].values))\n",
        "words_rest = set(list(data_rest['TOKEN'].values))\n",
        "words_hotels = set(list(data_hotels['TOKEN'].values))\n",
        "words = set({})\n",
        "words = words.union(words_lap,words_comp,words_router,words_speaker,words_rest,words_hotels)\n",
        "words.add('PADword')\n",
        "n_words = len(words)\n",
        "print(\"# of words in the datasets:\",n_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of words in the datasets: 12452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4yEkgZGepHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07febfc5-b3ed-4bf2-e1bf-3038a27f7a3c"
      },
      "source": [
        "#for the same reason we create the set containing the unique tags\n",
        "tags = list(set(data_lap[\"TAG\"].values))\n",
        "n_tags = len(tags)\n",
        "print(\"# of tags in the datasets:\",n_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of tags in the datasets: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMHBj6J5ew__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a8a2c1d2-b949-48c5-d58d-d05489ffb084"
      },
      "source": [
        "#we get all the sentences and we print the longest sentence\n",
        "getter_lap = SentenceGetter(data_lap)\n",
        "sentences_lap = getter_lap.sentences\n",
        "print(\"# of sentences in the laptop dataset:\",len(sentences_lap))\n",
        "\n",
        "getter_comp = SentenceGetter(data_comp)\n",
        "sentences_comp = getter_comp.sentences\n",
        "print(\"# of sentences in the computers dataset:\",len(sentences_comp))\n",
        "\n",
        "getter_router = SentenceGetter(data_router)\n",
        "sentences_router = getter_router.sentences\n",
        "print(\"# of sentences in the routers dataset:\",len(sentences_router))\n",
        "\n",
        "getter_speaker = SentenceGetter(data_speaker)\n",
        "sentences_speaker = getter_speaker.sentences\n",
        "print(\"# of sentences in the speakers dataset:\",len(sentences_speaker))\n",
        "\n",
        "getter_rest = SentenceGetter(data_rest)\n",
        "sentences_rest = getter_rest.sentences\n",
        "print(\"# of sentences in the restaurants dataset:\",len(sentences_rest))\n",
        "\n",
        "getter_hotels = SentenceGetter(data_hotels)\n",
        "sentences_hotels = getter_hotels.sentences\n",
        "print(\"# of sentences in the hotels dataset:\",len(sentences_hotels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of sentences in the laptop dataset: 3845\n",
            "# of sentences in the computers dataset: 531\n",
            "# of sentences in the routers dataset: 879\n",
            "# of sentences in the speakers dataset: 689\n",
            "# of sentences in the restaurants dataset: 4674\n",
            "# of sentences in the hotels dataset: 266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPuyQz3ye3c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "90199d11-21f7-4fbf-b733-7c11a4361430"
      },
      "source": [
        "#we print the first sentence in the above format\n",
        "print(sentences_router[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Affordable', 'O'), ('price', 'B-aspect'), (',', 'O'), ('reliable', 'O'), ('item', 'B-aspect'), ('and', 'O'), ('excellent', 'O'), ('customer', 'B-aspect'), ('service', 'I-aspect'), ('.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXXIDqy5fR15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5b619bd4-1e3a-43d5-d191-ebce9f8b2a4b"
      },
      "source": [
        "#we create the X dataset containing only the padded sentences' tokens\n",
        "max_len = 100\n",
        "print(\"First padded sentence of each dataset:\")\n",
        "X = [[w[0]for w in s] for s in sentences_lap]\n",
        "new_X_lap = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_lap.append(new_seq)\n",
        "print(new_X_lap[0])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_comp]\n",
        "new_X_comp = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_comp.append(new_seq)\n",
        "print(new_X_comp[0])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_router]\n",
        "new_X_router = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_router.append(new_seq)\n",
        "print(new_X_router[0])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_speaker]\n",
        "new_X_speaker = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_speaker.append(new_seq)\n",
        "print(new_X_speaker[0])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_rest]\n",
        "new_X_rest = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_rest.append(new_seq)\n",
        "print(new_X_rest[0])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_hotels]\n",
        "new_X_hotels = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_hotels.append(new_seq)\n",
        "print(new_X_hotels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First padded sentence of each dataset:\n",
            "['I', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'good', 'battery', 'life', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword']\n",
            "['I', 'purchased', 'this', 'monitor', 'because', 'of', 'budgetary', 'concerns', '.', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword']\n",
            "['Affordable', 'price', ',', 'reliable', 'item', 'and', 'excellent', 'customer', 'service', '.', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword']\n",
            "['These', 'speakers', 'are', 'incredibly', 'amazing', '.', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword']\n",
            "['Judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', ',', 'but', 'not', 'any', 'longer', '.', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword']\n",
            "['Room', 'Was', 'Acceptable', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword', 'PADword']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO_sXDFHfe5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we also create the padded y \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tags2index = {\"O\":0,\"B-aspect\":1,\"I-aspect\":2}\n",
        "y_lap = [[tags2index[w[1]] for w in s] for s in sentences_lap]\n",
        "y_lap = pad_sequences(maxlen=max_len, sequences=y_lap, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_comp = [[tags2index[w[1]] for w in s] for s in sentences_comp]\n",
        "y_comp = pad_sequences(maxlen=max_len, sequences=y_comp, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_router = [[tags2index[w[1]] for w in s] for s in sentences_router]\n",
        "y_router = pad_sequences(maxlen=max_len, sequences=y_router, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_speaker = [[tags2index[w[1]] for w in s] for s in sentences_speaker]\n",
        "y_speaker = pad_sequences(maxlen=max_len, sequences=y_speaker, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_rest = [[tags2index[w[1]] for w in s] for s in sentences_rest]\n",
        "y_rest = pad_sequences(maxlen=max_len, sequences=y_rest, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_hotels = [[tags2index[w[1]] for w in s] for s in sentences_hotels]\n",
        "y_hotels = pad_sequences(maxlen=max_len, sequences=y_hotels, padding=\"post\", value=tags2index[\"O\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj0BYjeeflPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "26ea25f5-aafe-4702-db8d-4e2e9b0ea137"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /tensorflow-1.15.2/python3.6 (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdi-vP9ffrXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers.merge import add\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN9kX7unfvEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We need to generate our training, validation and test set\n",
        "#The idea is to train on 5 domains and test on the remaining one\n",
        "#We split all the six dataset into training and test set\n",
        "X_tr_lap, X_te_lap, y_tr_lap, y_te_lap = train_test_split(new_X_lap, y_lap, test_size=0.2, random_state=2018)\n",
        "X_tr_comp, X_te_comp, y_tr_comp, y_te_comp = train_test_split(new_X_comp, y_comp, test_size=0.2, random_state=2018)\n",
        "X_tr_router, X_te_router, y_tr_router, y_te_router = train_test_split(new_X_router, y_router, test_size=0.2, random_state=2018)\n",
        "X_tr_speaker, X_te_speaker, y_tr_speaker, y_te_speaker = train_test_split(new_X_speaker, y_speaker, test_size=0.2, random_state=2018)\n",
        "X_tr_rest, X_te_rest, y_tr_rest, y_te_rest = train_test_split(new_X_rest, y_rest, test_size=0.2, random_state=2018)\n",
        "X_tr_hotels, X_te_hotels, y_tr_hotels, y_te_hotels = train_test_split(new_X_hotels, y_hotels, test_size=0.2, random_state=2018)\n",
        "\n",
        "#We concateate the training set of five datasets\n",
        "X_tr = X_tr_rest + X_tr_hotels + X_tr_comp + X_tr_lap + X_tr_router\n",
        "X_val = X_te_rest + X_te_hotels + X_te_comp + X_te_lap + X_te_router\n",
        "y_tr = np.concatenate((y_tr_rest, y_tr_hotels, y_tr_comp, y_tr_lap, y_tr_router), axis=0)\n",
        "y_val = np.concatenate((y_te_rest, y_te_hotels, y_te_comp, y_te_lap, y_te_router), axis=0)\n",
        "\n",
        "#The remaining one will be used as test set\n",
        "X_te = X_tr_speaker + X_te_speaker\n",
        "y_te = np.concatenate((y_tr_speaker, y_te_speaker), axis=0)\n",
        "\n",
        "#If you are interested in testing on one of the datasets used for training, you only need to switch the variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPEzLvHFifCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We take only a number of sentences that is a multple of 32 (we don't consider some sentences)\n",
        "X_tr, X_val = X_tr[:(int(len(X_tr)/32))*32], X_val[:(int(len(X_val)/32))*32]\n",
        "y_tr, y_val = y_tr[:(int(len(y_tr)/32))*32], y_val[:(int(len(y_val)/32))*32]\n",
        "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
        "y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n",
        "\n",
        "#we add some sentences to the training set containing only padwords\n",
        "for i in range(32 - len(X_te)%32):\n",
        "  new_seq = []\n",
        "  for j in range(max_len):\n",
        "    new_seq.append(\"PADword\")\n",
        "  X_te.append(new_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1VKqtPzi78s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "1926b5c0-fea7-49c1-bd1b-ebf01e5ee4c5"
      },
      "source": [
        "for i in range(5):\n",
        "  #we define the structure of the network and we print the network characteristics\n",
        "  input_text = Input(shape=(max_len,), dtype=tf.string)\n",
        "  embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\n",
        "  x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
        "  x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "  x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "  out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
        "  model = Model(input_text, out)\n",
        "  model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  \n",
        "\n",
        "  print(\"Run n° \" + str(i+1))\n",
        "\n",
        "  #fitting the model\n",
        "  history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),batch_size=batch_size, epochs=10, verbose=1)\n",
        "  \n",
        "  #prediction on the test set\n",
        "  test_pred = model.predict(np.array(X_te), verbose=1)\n",
        "  pred_labels = pred2label(test_pred)\n",
        "  test_labels = test2label(y_te)\n",
        "\n",
        "  #print the classification report\n",
        "  print(classification_report(test_labels,pred_labels[:len(new_X_speaker)]))\n",
        "\n",
        "  #print the f1 of each sentence on a file for a future statistical analysis \n",
        "  print_f1_on_file(\"/content/drive/My Drive/Tesi_ABSA/speaker_elmo_full_f1_\"+str(i+1)+\".txt\",test_labels,pred_labels[:len(new_X_speaker)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
            "Traceback (most recent call last):\n",
            "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run n° 1\n",
            "Train on 8128 samples, validate on 2016 samples\n",
            "Epoch 1/10\n",
            " 192/8128 [..............................] - ETA: 2:02:51 - loss: 0.3143 - accuracy: 0.9079"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f3ee60005c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m#prediction on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6SD5VS2kX3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvqoapS0jj-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}