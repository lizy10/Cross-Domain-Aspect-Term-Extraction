{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATE_Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3tN0pVqvPPP",
        "colab_type": "text"
      },
      "source": [
        "# Single Domain Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7f2G8Tvm0hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to extract the aspects from the sentence and print the f1 for each sentence in a file\n",
        "def print_f1_on_file(filename,true_aspects,extracted_aspects):\n",
        "  count = 0\n",
        "  f1 = 0.0\n",
        "  f = open(filename,\"w+\")\n",
        "  for ea,ta in zip(extracted_aspects,true_aspects):\n",
        "    if 'B-aspect' in ta:\n",
        "      f.write(str(f1_score([ta],[ea]))+\"\\n\")\n",
        "      count += 1\n",
        "      f1 += f1_score([ta],[ea])\n",
        "  f.close()\n",
        "  print(f1)\n",
        "  print(count)\n",
        "  print(f1/count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOFuKxCbnB_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "\n",
        "import os\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zspnC1OOnKHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Connessione a drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SECR05BNnLGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#class for get sentences in the format of tuples [(TOKEN,TAG),(TOKEN,TAG)...]\n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"TOKEN\"].values.tolist(),s[\"TAG\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"SENTENCE\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uuOyQDynTEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download of pre-trained word2vec model\n",
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThtHfRQjn8Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#converting the downloaded model into a dictionary {word:vector}\n",
        "import numpy as np\n",
        "filepath = \"GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "embeddings = {}\n",
        "from gensim.models import KeyedVectors\n",
        "print(\"Loading the Word2Vec model...\")\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(filepath, binary=True) \n",
        "for word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n",
        "    coefs = np.asarray(vector, dtype='float32')\n",
        "    embeddings[word] = coefs\n",
        "print('# vectors:',  len(embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gcLEBKYoLtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We load all the datasets\n",
        "data_rest = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/restaurants1.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_lap = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/laptops_2014.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_hotels = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/hotels.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_comp = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/Computer.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_speaker = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/Speaker.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_router = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/Router.csv', encoding=\"latin-1\",sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bet1NxKuopTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We read all the sentences in the datasets\n",
        "sentences_rest = SentenceGetter(data_rest).sentences\n",
        "sentences_lap = SentenceGetter(data_lap).sentences\n",
        "sentences_hotels = SentenceGetter(data_hotels).sentences\n",
        "sentences_comp = SentenceGetter(data_comp).sentences\n",
        "sentences_speaker = SentenceGetter(data_speaker).sentences\n",
        "sentences_router = SentenceGetter(data_router).sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOSNUrJvoyeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We create the X and y sets for each domain\n",
        "max_len = 85\n",
        "sentences_words_rest = [[str(w[0]) for w in s] for s in sentences_rest]\n",
        "sentences_tags_rest = [[w[1] for w in s] for s in sentences_rest]\n",
        "sentences_words_lap = [[str(w[0]) for w in s] for s in sentences_lap]\n",
        "sentences_tags_lap = [[w[1] for w in s] for s in sentences_lap]\n",
        "sentences_words_hotels = [[str(w[0]) for w in s] for s in sentences_hotels]\n",
        "sentences_tags_hotels = [[w[1] for w in s] for s in sentences_hotels]\n",
        "sentences_words_comp = [[str(w[0]) for w in s] for s in sentences_comp]\n",
        "sentences_tags_comp = [[w[1] for w in s] for s in sentences_comp]\n",
        "sentences_words_speaker = [[str(w[0]) for w in s] for s in sentences_speaker]\n",
        "sentences_tags_speaker = [[w[1] for w in s] for s in sentences_speaker]\n",
        "sentences_words_router = [[str(w[0]) for w in s] for s in sentences_router]\n",
        "sentences_tags_router = [[w[1] for w in s] for s in sentences_router]\n",
        "\n",
        "#We create a list with all the words in the six datasets\n",
        "sentences_words = sentences_words_rest + sentences_words_lap + sentences_words_hotels + sentences_words_comp + sentences_words_speaker + sentences_words_router\n",
        "sentences_tags = sentences_tags_rest + sentences_tags_lap +sentences_tags_hotels +sentences_tags_comp+ sentences_tags_speaker+sentences_tags_router"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNleH8JppDx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for converting all the tags into indeces\n",
        "def convert_tags_to_id(tags):\n",
        "  indexes = []\n",
        "  for tag in tags:\n",
        "    i = []\n",
        "    for t in tag:\n",
        "      if t==\"O\":\n",
        "        i.append(0)\n",
        "      elif t==\"I-aspect\":\n",
        "        i.append(1)\n",
        "      elif t=='B-aspect':\n",
        "        i.append(2)\n",
        "    indexes.append(i)\n",
        "  return indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxw-LCjAqRGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw2SPj9TqPll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from keras import backend as K\n",
        "from keras.models import Model, Input\n",
        "from keras.layers.merge import add\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnoLHeqVpUTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We perform a 5-fold cross validation on the restaurant datasets\n",
        "cv = KFold(n_splits=5, random_state=42, shuffle=False)\n",
        "run = 0\n",
        "for train_index, test_index in cv.split(sentences_words_rest):\n",
        "  run += 1\n",
        "\n",
        "  #creation of training and test set\n",
        "  print(\"Run n° \", run)\n",
        "  train_sentences_words = list(pd.Series(sentences_words_rest)[train_index])\n",
        "  train_sentences_tags = list(pd.Series(sentences_tags_rest)[train_index])\n",
        "  test_sentences_words = list(pd.Series(sentences_words_rest)[test_index])\n",
        "  test_sentences_tags = list(pd.Series(sentences_tags_rest)[test_index])\n",
        "  sentences_words = train_sentences_words + test_sentences_words\n",
        "\n",
        "  #Generation of vocabulary and tags\n",
        "  vocab = set(itertools.chain(*[[w for w in s] for s in train_sentences_words])) \n",
        "  tags = set(itertools.chain(*[[w for w in s] for s in train_sentences_tags]))\n",
        "  sentenecs_lens = map(len, train_sentences_words)\n",
        "  MAX_LEN = 85\n",
        "  VOCAB_SIZE = len(vocab)\n",
        "\n",
        "  #We define a mapping between words and indeces\n",
        "  words_tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters=[], oov_token='__UNKNOWN__')\n",
        "  words_tokenizer.fit_on_texts(map(lambda s: ' '.join(s), sentences_words))\n",
        "  word_index = words_tokenizer.word_index\n",
        "  print(word_index)\n",
        "  word_index['__PADDING__'] = 0\n",
        "  index_word = {i:w for w, i in word_index.items()}\n",
        "  print('Unique tokens:', len(word_index))\n",
        "\n",
        "  #we define train and test sequences and convert the tags into indeces\n",
        "  train_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), train_sentences_words))\n",
        "  test_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), test_sentences_words))\n",
        "  train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LEN)\n",
        "  test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LEN)\n",
        "  tags_tokenizer = Tokenizer(num_words=len(tags), filters='', oov_token='__UNKNOWN__', lower=False)\n",
        "  tags_tokenizer.fit_on_texts(map(lambda s: ' '.join(s), train_sentences_tags))\n",
        "  tag_index = tags_tokenizer.word_index\n",
        "  tag_index['__PADDING__'] = 0\n",
        "  index_tag = {i:w for w, i in tag_index.items()}\n",
        "  index_tag_wo_padding = dict(index_tag)\n",
        "  index_tag_wo_padding[tag_index['__PADDING__']] = '0'\n",
        "  print('Unique tags:', len(tag_index))\n",
        "  train_tags = []\n",
        "  test_tags = []\n",
        "  train_tags = convert_tags_to_id(train_sentences_tags)\n",
        "  test_tags = convert_tags_to_id(test_sentences_tags)\n",
        "  train_tags_padded = pad_sequences(train_tags, maxlen=MAX_LEN)\n",
        "  test_tags_padded = pad_sequences(test_tags, maxlen=MAX_LEN)\n",
        "  train_tags_padded = np.expand_dims(train_tags_padded, -1)\n",
        "  test_tags_padded = np.expand_dims(test_tags_padded, -1)\n",
        "  \n",
        "  #We create an embedding matrix containing only the words in our training set\n",
        "  num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
        "  embedding_matrix = np.zeros((num_words, 300))\n",
        "  for word, i in word_index.items():\n",
        "    if i >= VOCAB_SIZE:\n",
        "      continue\n",
        "    embedding_vector = embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  #We define our model\n",
        "  pretrained_embedding_layer = Embedding(num_words,\n",
        "                                300,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_LEN,\n",
        "                                trainable=True)\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "  embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
        "  x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(embedded_sequences)\n",
        "  x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "  x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "  out = TimeDistributed(Dense(len(tag_index), activation=\"softmax\"))(x)\n",
        "  model = Model(sequence_input, out)\n",
        "  model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  \n",
        "  #fitting the model\n",
        "  model.fit(train_sequences_padded[:((int)(len(train_sentences_words)*0.8))], train_tags_padded[:((int)(len(train_sentences_words)*0.8))],\n",
        "            batch_size=32,\n",
        "            epochs=10,\n",
        "            validation_data=(train_sequences_padded[-((int)(len(train_sentences_words)*0.2)):], train_tags_padded[-((int)(len(train_sentences_words)*0.2)):]))\n",
        "  \n",
        "  #prediction on the test set\n",
        "  lstm_predicted = model.predict(test_sequences_padded)\n",
        "  lstm_predicted_tags = []\n",
        "  index_tag_wo_padding = {0:\"O\",1:\"I-aspect\", 2:\"B-aspect\"}\n",
        "  for s, s_pred in zip(test_sentences_words, lstm_predicted):\n",
        "    tags = np.argmax(s_pred, axis=1)\n",
        "    tags = list(map(index_tag_wo_padding.get,tags))[-len(s):]\n",
        "    lstm_predicted_tags.append(tags)\n",
        "  \n",
        "  #print the performances\n",
        "  print(classification_report(test_sentences_tags, lstm_predicted_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IjQYOqOxvQ3",
        "colab_type": "text"
      },
      "source": [
        "# Cross-Domain Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ7gr5yhqLSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We split the six datasets into training and test set\n",
        "train_sentences_words_rest, test_sentences_words_rest, train_sentences_tags_rest, test_sentences_tags_rest = train_test_split(sentences_words_rest, sentences_tags_rest, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_lap, test_sentences_words_lap, train_sentences_tags_lap, test_sentences_tags_lap = train_test_split(sentences_words_lap, sentences_tags_lap, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_hotels, test_sentences_words_hotels, train_sentences_tags_hotels, test_sentences_tags_hotels = train_test_split(sentences_words_hotels, sentences_tags_hotels, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_comp, test_sentences_words_comp, train_sentences_tags_comp, test_sentences_tags_comp = train_test_split(sentences_words_comp, sentences_tags_comp, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_speaker, test_sentences_words_speaker, train_sentences_tags_speaker, test_sentences_tags_speaker = train_test_split(sentences_words_speaker, sentences_tags_speaker, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_router, test_sentences_words_router, train_sentences_tags_router, test_sentences_tags_router = train_test_split(sentences_words_router, sentences_tags_router, test_size=0.2, random_state=2018)\n",
        "\n",
        "#We concatenate 5 of the 6 datasets\n",
        "train_sentences_words = train_sentences_words_comp + train_sentences_words_router + train_sentences_words_lap + train_sentences_words_rest + train_sentences_words_speaker\n",
        "train_sentences_tags = train_sentences_tags_comp + train_sentences_tags_router + train_sentences_tags_lap + train_sentences_tags_rest + train_sentences_tags_speaker\n",
        "val_sentences_words = test_sentences_words_comp + test_sentences_words_router + test_sentences_words_lap +test_sentences_words_rest + test_sentences_words_speaker\n",
        "val_sentences_tags = test_sentences_tags_comp + test_sentences_tags_router + test_sentences_tags_lap + test_sentences_tags_rest +test_sentences_tags_speaker\n",
        "\n",
        "#The remaining dataset is used as test set\n",
        "test_sentences_words = train_sentences_words_hotels + test_sentences_words_hotels\n",
        "test_sentences_tags = train_sentences_tags_hotels + test_sentences_tags_hotels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XeTskwdvuFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We follow the same steps of the single domain experiments\n",
        "#We perform five iterations to handle the problem of weights that are randomly instantiated\n",
        "vocab = set(itertools.chain(*[[w for w in s] for s in sentences_words])) \n",
        "tags = set(itertools.chain(*[[w for w in s] for s in sentences_tags]))\n",
        "for j in range(5):\n",
        "  j += 1\n",
        "  print(\"Run n° \", j)\n",
        "\n",
        "  sentenecs_lens = map(len, train_sentences_words)\n",
        "  MAX_LEN = 75#max(sentenecs_lens)\n",
        "  VOCAB_SIZE = len(vocab)\n",
        "  words_tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters=[], oov_token='__UNKNOWN__')\n",
        "  words_tokenizer.fit_on_texts(map(lambda s: ' '.join(s), sentences_words))\n",
        "  word_index = words_tokenizer.word_index\n",
        "  word_index['__PADDING__'] = 0\n",
        "  index_word = {i:w for w, i in word_index.items()}\n",
        "  print('Unique tokens:', len(word_index))\n",
        "  #we define train and test sequences\n",
        "  train_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), train_sentences_words))\n",
        "  test_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), test_sentences_words))\n",
        "  val_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), val_sentences_words))\n",
        "  train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LEN)\n",
        "  test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LEN)\n",
        "  val_sequences_padded = pad_sequences(val_sequences, maxlen=MAX_LEN)\n",
        "\n",
        "  tags_tokenizer = Tokenizer(num_words=len(tags), filters='', oov_token='__UNKNOWN__', lower=False)\n",
        "  tags_tokenizer.fit_on_texts(map(lambda s: ' '.join(s), train_sentences_tags))\n",
        "  tag_index = {\"O\":0, \"I-aspect\":1,\"B-aspect\":2}\n",
        "  index_tag = {i:w for w, i in tag_index.items()}\n",
        "\n",
        "\n",
        "  print('Unique tags:', len(tag_index))\n",
        "  train_tags = []\n",
        "  test_tags = []\n",
        "  train_tags = convert_tags_to_id(train_sentences_tags)\n",
        "  test_tags = convert_tags_to_id(test_sentences_tags)\n",
        "  val_tags = convert_tags_to_id(val_sentences_tags)\n",
        "  train_tags_padded = pad_sequences(train_tags, maxlen=MAX_LEN)\n",
        "  test_tags_padded = pad_sequences(test_tags, maxlen=MAX_LEN)\n",
        "  val_tags_padded = pad_sequences(val_tags, maxlen=MAX_LEN)\n",
        "  \n",
        "  train_tags_padded = np.expand_dims(train_tags_padded, -1)\n",
        "  test_tags_padded = np.expand_dims(test_tags_padded, -1)\n",
        "  val_tags_padded = np.expand_dims(val_tags_padded, -1)\n",
        "\n",
        "  num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
        "  embedding_matrix = np.zeros((num_words, 300))\n",
        "  for word, i in word_index.items():\n",
        "    if i >= VOCAB_SIZE:\n",
        "      continue\n",
        "    embedding_vector = embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  print(num_words)\n",
        "  pretrained_embedding_layer = Embedding(num_words,\n",
        "                                300,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_LEN,\n",
        "                                trainable=True)\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "  embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
        "  x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(embedded_sequences)\n",
        "  x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "  x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "  out = TimeDistributed(Dense(len(tag_index), activation=\"softmax\"))(x)\n",
        "  model = Model(sequence_input, out)\n",
        "  model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit(train_sequences_padded, train_tags_padded,\n",
        "            batch_size=32,\n",
        "            epochs=10,\n",
        "            validation_data=(val_sequences_padded, val_tags_padded))\n",
        "  lstm_predicted = model.predict(test_sequences_padded)\n",
        "  lstm_predicted_tags = []\n",
        "  index_tag_wo_padding = {0:\"O\",1:\"I-aspect\", 2:\"B-aspect\"}\n",
        "  for s, s_pred in zip(test_sentences_words, lstm_predicted):\n",
        "    tags = np.argmax(s_pred, axis=1)\n",
        "    tags = list(map(index_tag_wo_padding.get,tags))[-len(s):]\n",
        "    lstm_predicted_tags.append(tags)\n",
        "  for s,s1 in zip(lstm_predicted_tags[0],test_sentences_tags[0]):\n",
        "    print(s, \" \", s1)\n",
        "  print(classification_report(test_sentences_tags, lstm_predicted_tags))\n",
        "  print_f1_on_file(\"/content/drive/My Drive/Tesi_ABSA/hotels_w2v_full_f1_\"+str(j+1)+\".txt\",test_sentences_tags,lstm_predicted_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jaJHnJBxSJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}