{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aspect_Extraction_from_YELP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amDPb7vOBktQ",
        "colab_type": "text"
      },
      "source": [
        "# Training of the model on all the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6MuOWumAbX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we first connect to my google drive in order to collect all the data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOSTYjk4CEDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYqhvft2CXFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we load all the datasets \n",
        "data_lap = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/laptops_2014.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_lap = data_lap.fillna(method=\"ffill\")\n",
        "\n",
        "data_comp = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Computer.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_comp = data_comp.fillna(method=\"ffill\")\n",
        "\n",
        "data_router = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Router.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_router = data_router.fillna(method=\"ffill\")\n",
        "\n",
        "data_speaker = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/Speaker.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_speaker = data_speaker.fillna(method=\"ffill\")\n",
        "\n",
        "data_rest = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/restaurants1.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_rest = data_rest.fillna(method=\"ffill\")\n",
        "\n",
        "data_hotels = pd.read_csv(\"/content/drive/My Drive/Tesi_ABSA/dataset/hotels.csv\", encoding=\"latin1\", sep=\"\\t\")\n",
        "data_hotels = data_hotels.fillna(method=\"ffill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg8W4WFuCdYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creation of a set containing the words\n",
        "\n",
        "words_lap = set(list(data_lap['TOKEN'].values))\n",
        "words_comp = set(list(data_comp['TOKEN'].values))\n",
        "words_router = set(list(data_router['TOKEN'].values))\n",
        "words_speaker = set(list(data_speaker['TOKEN'].values))\n",
        "words_rest = set(list(data_rest['TOKEN'].values))\n",
        "words_hotels = set(list(data_hotels['TOKEN'].values))\n",
        "words = set({})\n",
        "words = words.union(words_lap,words_comp,words_router,words_speaker,words_rest,words_hotels)\n",
        "words.add('PADword')\n",
        "n_words = len(words)\n",
        "print(n_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbFlfTVfCiep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we create the set containing the unique tags\n",
        "tags = list(set(data_lap[\"TAG\"].values))\n",
        "n_tags = len(tags)\n",
        "print(n_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7nKgarcCkhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#class for get sentences in the format of tuples [(TOKEN,TAG),(TOKEN,TAG)...]\n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"TOKEN\"].values.tolist(),s[\"TAG\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"SENTENCE\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHJ8vskFCsAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we get all the sentences and we print the longest sentence\n",
        "getter_lap = SentenceGetter(data_lap)\n",
        "sentences_lap = getter_lap.sentences\n",
        "print(len(sentences_lap))\n",
        "\n",
        "getter_comp = SentenceGetter(data_comp)\n",
        "sentences_comp = getter_comp.sentences\n",
        "print(len(sentences_comp))\n",
        "\n",
        "getter_router = SentenceGetter(data_router)\n",
        "sentences_router = getter_router.sentences\n",
        "print(len(sentences_router))\n",
        "\n",
        "getter_speaker = SentenceGetter(data_speaker)\n",
        "sentences_speaker = getter_speaker.sentences\n",
        "print(len(sentences_speaker))\n",
        "\n",
        "getter_rest = SentenceGetter(data_rest)\n",
        "sentences_rest = getter_rest.sentences\n",
        "print(len(sentences_rest))\n",
        "\n",
        "getter_hotels = SentenceGetter(data_hotels)\n",
        "sentences_hotels = getter_hotels.sentences\n",
        "print(len(sentences_hotels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZCqXPzFCtVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we print the first sentence in the above format\n",
        "print(sentences_router[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoshI9Q5Cu0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we create the X dataset containing only the padded sentences' tokens\n",
        "max_len = 100\n",
        "X = [[w[0]for w in s] for s in sentences_lap]\n",
        "new_X_lap = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_lap.append(new_seq)\n",
        "print(new_X_lap[15])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_comp]\n",
        "new_X_comp = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_comp.append(new_seq)\n",
        "print(new_X_comp[15])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_router]\n",
        "new_X_router = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_router.append(new_seq)\n",
        "print(new_X_router[15])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_speaker]\n",
        "new_X_speaker = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_speaker.append(new_seq)\n",
        "print(new_X_speaker[15])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_rest]\n",
        "new_X_rest = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_rest.append(new_seq)\n",
        "print(new_X_rest[15])\n",
        "\n",
        "X = [[w[0]for w in s] for s in sentences_hotels]\n",
        "new_X_hotels = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X_hotels.append(new_seq)\n",
        "print(new_X_hotels[15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhVe40myDEk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we also create the padded y \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tags2index = {\"O\":0,\"B-aspect\":1,\"I-aspect\":2}\n",
        "y_lap = [[tags2index[w[1]] for w in s] for s in sentences_lap]\n",
        "y_lap = pad_sequences(maxlen=max_len, sequences=y_lap, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_comp = [[tags2index[w[1]] for w in s] for s in sentences_comp]\n",
        "y_comp = pad_sequences(maxlen=max_len, sequences=y_comp, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_router = [[tags2index[w[1]] for w in s] for s in sentences_router]\n",
        "y_router = pad_sequences(maxlen=max_len, sequences=y_router, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_speaker = [[tags2index[w[1]] for w in s] for s in sentences_speaker]\n",
        "y_speaker = pad_sequences(maxlen=max_len, sequences=y_speaker, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_rest = [[tags2index[w[1]] for w in s] for s in sentences_rest]\n",
        "y_rest = pad_sequences(maxlen=max_len, sequences=y_rest, padding=\"post\", value=tags2index[\"O\"])\n",
        "\n",
        "y_hotels = [[tags2index[w[1]] for w in s] for s in sentences_hotels]\n",
        "y_hotels = pad_sequences(maxlen=max_len, sequences=y_hotels, padding=\"post\", value=tags2index[\"O\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnRAb90zDGvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr_lap, X_te_lap, y_tr_lap, y_te_lap = train_test_split(new_X_lap, y_lap, test_size=0.2, random_state=2018)\n",
        "X_tr_comp, X_te_comp, y_tr_comp, y_te_comp = train_test_split(new_X_comp, y_comp, test_size=0.2, random_state=2018)\n",
        "X_tr_router, X_te_router, y_tr_router, y_te_router = train_test_split(new_X_router, y_router, test_size=0.2, random_state=2018)\n",
        "X_tr_speaker, X_te_speaker, y_tr_speaker, y_te_speaker = train_test_split(new_X_speaker, y_speaker, test_size=0.2, random_state=2018)\n",
        "X_tr_rest, X_te_rest, y_tr_rest, y_te_rest = train_test_split(new_X_rest, y_rest, test_size=0.2, random_state=2018)\n",
        "X_tr_hotels, X_te_hotels, y_tr_hotels, y_te_hotels = train_test_split(new_X_hotels, y_hotels, test_size=0.2, random_state=2018)\n",
        "X_tr = X_tr_hotels + X_tr_speaker + X_tr_comp + X_tr_lap + X_tr_router\n",
        "X_val = X_te_hotels + X_te_speaker + X_te_comp + X_te_lap + X_te_router\n",
        "y_tr = np.concatenate((y_tr_hotels, y_tr_speaker, y_tr_comp, y_tr_lap, y_tr_router), axis=0)\n",
        "y_val = np.concatenate((y_te_hotels, y_te_speaker, y_te_comp, y_te_lap, y_te_router), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MeGO7nlDi-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mUeqxKJDUQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we define the function that will be used in the embedding layer of the network\n",
        "batch_size = 32\n",
        "def ElmoEmbedding(x):\n",
        "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x,    tf.string)),\"sequence_len\": tf.constant(batch_size*[max_len])\n",
        "                     },\n",
        "                      signature=\"tokens\",\n",
        "                      as_dict=True)[\"elmo\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKlJMfQ9Da8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model, Input\n",
        "from keras.layers.merge import add\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
        "input_text = Input(shape=(max_len,), dtype=tf.string)\n",
        "embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\n",
        "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
        "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
        "model = Model(input_text, out)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBkJmlP3Ded3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we train the model \n",
        "X_tr, X_val = X_tr[:(int(len(X_tr)/32))*32], X_val[:(int(len(X_val)/32))*32]\n",
        "y_tr, y_val = y_tr[:(int(len(y_tr)/32))*32], y_val[:(int(len(y_val)/32))*32]\n",
        "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
        "y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n",
        "\n",
        "history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),batch_size=batch_size, epochs=10, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLl1QELzEDmk",
        "colab_type": "text"
      },
      "source": [
        "# ABSA on Yelp Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvuokLZdEK6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install twython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV1A9_tDDpAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save_weights(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/model_weights.weights\")\n",
        "model.load_weights('/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/model_weights.weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjGtCyQVD64v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evWPJXfSEPlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We first need to divide reviews into atomic sentences\n",
        "#Then we also extract the sentiment of the sentence in order to assign it to all the aspects in the sentence\n",
        "\n",
        "f_rev = open(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/yelp_training_set_review.json\",\"r\")\n",
        "f_rev_output = open(\"/content/drive/My Drive/Tesi_ABSA/dataset/Aspect Extraction Task/review_divided.txt\",\"w+\")\n",
        "count = 0\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "#We iterate over all the rows in the yelp dataset (each line is a json)\n",
        "for line in f_rev:\n",
        "  #we read the text of the review\n",
        "  rev_text = json.loads(line)[\"text\"]\n",
        "  #we read the user_id\n",
        "  rev_user = json.loads(line)[\"user_id\"]\n",
        "  #we read the business_id\n",
        "  rev_business = json.loads(line)[\"business_id\"]\n",
        "  #we tokenize the sentence using the sent_tokenize of nltk\n",
        "  rev_splitted = sent_tokenize(rev_text)\n",
        "\n",
        "  #splitted sentences\n",
        "  res = []\n",
        "  #sentiment scores\n",
        "  compounds = []\n",
        "  neg = []\n",
        "  neu = []\n",
        "  pos = []\n",
        "  #We iterate over the sentences in the current review and predict the sentiment scores\n",
        "  for i in range (len(rev_splitted)):\n",
        "    ss = sid.polarity_scores(rev_splitted[i])\n",
        "    res.append(rev_splitted[i])\n",
        "    compounds.append(ss['compound'])\n",
        "    neg.append(ss['neg'])\n",
        "    neu.append(ss['neu'])\n",
        "    pos.append(ss['pos'])\n",
        "  sentence_id = 0\n",
        "  #We write all the extracted values on the f_rev_output file (each value is separated by \\t)\n",
        "  for sentence,comp,nega,neut,posi in zip(res,compounds,neg,neu,pos):\n",
        "    f_rev_output.write(str(count)+\"\\t\")\n",
        "    f_rev_output.write(str(sentence_id)+\"\\t\")\n",
        "    f_rev_output.write(str(comp)+\"\\t\")\n",
        "    f_rev_output.write(str(nega)+\"\\t\")\n",
        "    f_rev_output.write(str(neut)+\"\\t\")\n",
        "    f_rev_output.write(str(posi)+\"\\t\")\n",
        "    f_rev_output.write(rev_user+\"\\t\")\n",
        "    f_rev_output.write(rev_business+\"\\t\")\n",
        "    splitted_sentence = re.findall(r\"[\\w']+|[.,!?;]\", sentence)\n",
        "    for token in splitted_sentence:\n",
        "      f_rev_output.write(token+\" \")\n",
        "    f_rev_output.write(\"\\n\")\n",
        "    sentence_id += 1\n",
        "    \n",
        "  count += 1\n",
        "f_rev_output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh-jsjsaFZNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}