{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATE_GloVe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1oM5NCKyQcM",
        "colab_type": "text"
      },
      "source": [
        "# Single Domain Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZvMfV7Sxb7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to extract the aspects from the sentence \n",
        "def print_f1_on_file(filename,true_aspects,extracted_aspects):\n",
        "  count = 0\n",
        "  f1 = 0.0\n",
        "  f = open(filename,\"w+\")\n",
        "  for ea,ta in zip(extracted_aspects,true_aspects):\n",
        "    if 'B-aspect' in ta:\n",
        "      f.write(str(f1_score([ta],[ea]))+\"\\n\")\n",
        "      count += 1\n",
        "      f1 += f1_score([ta],[ea])\n",
        "  f.close()\n",
        "  print(f1)\n",
        "  print(count)\n",
        "  print(f1/count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frBfySX8yUbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5MfAJDB0T0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev8vUbbeyYLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#class for get sentences in the format of tuples [(TOKEN,TAG),(TOKEN,TAG)...]\n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"TOKEN\"].values.tolist(),s[\"TAG\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"SENTENCE\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak2u7m5Oyi4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I now try another word embedding technique (GloVe embeddings)\n",
        "#in this section we download the pre-trained embeddings from stanford website\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR0uvOk5yj6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip glove.6B.zip\n",
        "!rm glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXv8aGwrylPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We load all the datasets\n",
        "data_rest = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/restaurants1.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_lap = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/laptops_2014.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_hotels = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/hotels.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_comp = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/Computer.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_speaker = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/Speaker.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "data_router = pd.read_csv('/content/drive/My Drive/Tesi_ABSA/dataset/Router.csv', encoding=\"latin-1\",sep=\"\\t\")\n",
        "\n",
        "#we get all the sentences and we print the longest sentence\n",
        "sentences_rest = SentenceGetter(data_rest).sentences\n",
        "sentences_lap = SentenceGetter(data_lap).sentences\n",
        "sentences_hotels = SentenceGetter(data_hotels).sentences\n",
        "sentences_comp = SentenceGetter(data_comp).sentences\n",
        "sentences_speaker = SentenceGetter(data_speaker).sentences\n",
        "sentences_router = SentenceGetter(data_router).sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBdHVIIwyqeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We create the X and y sets for all the datesets\n",
        "max_len = 85\n",
        "sentences_words_rest = [[str(w[0]) for w in s] for s in sentences_rest][:600]\n",
        "sentences_tags_rest = [[w[1] for w in s] for s in sentences_rest][:600]\n",
        "sentences_words_lap = [[str(w[0]) for w in s] for s in sentences_lap][:600]\n",
        "sentences_tags_lap = [[w[1] for w in s] for s in sentences_lap][:600]\n",
        "sentences_words_hotels = [[str(w[0]) for w in s] for s in sentences_hotels][:600]\n",
        "sentences_tags_hotels = [[w[1] for w in s] for s in sentences_hotels][:600]\n",
        "sentences_words_comp = [[str(w[0]) for w in s] for s in sentences_comp][:600]\n",
        "sentences_tags_comp = [[w[1] for w in s] for s in sentences_comp][:600]\n",
        "sentences_words_speaker = [[str(w[0]) for w in s] for s in sentences_speaker][:600]\n",
        "sentences_tags_speaker = [[w[1] for w in s] for s in sentences_speaker][:600]\n",
        "sentences_words_router = [[str(w[0]) for w in s] for s in sentences_router]\n",
        "sentences_tags_router = [[w[1] for w in s] for s in sentences_router]\n",
        "sentences_words = sentences_words_rest + sentences_words_hotels + sentences_words_comp + sentences_words_speaker + sentences_words_router\n",
        "sentences_tags = sentences_tags_rest +sentences_tags_hotels +sentences_tags_comp+ sentences_tags_speaker+sentences_tags_router"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAeq1tsjyw13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCg65oXnyyJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from keras import backend as K\n",
        "from keras.models import Model, Input\n",
        "from keras.layers.merge import add\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO3_1cS4yzdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We convert the embedding model into a dictionary\n",
        "embeddings = {}\n",
        "with open('glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings[word] = coefs\n",
        "\n",
        "print('# vectors:',  len(embeddings))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uop7iBizy3an",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for converting tags into indeces\n",
        "def convert_tags_to_id(tags):\n",
        "  indexes = []\n",
        "  for tag in tags:\n",
        "    i = []\n",
        "    for t in tag:\n",
        "      if t==\"O\":\n",
        "        i.append(0)\n",
        "      elif t==\"I-aspect\":\n",
        "        i.append(1)\n",
        "      elif t=='B-aspect':\n",
        "        i.append(2)\n",
        "    indexes.append(i)\n",
        "  return indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ7xtfosy7mC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The approach is the same of the word2vec notebook\n",
        "cv = KFold(n_splits=5, random_state=42, shuffle=False)\n",
        "run = 0\n",
        "for train_index, test_index in cv.split(sentences_words_rest):\n",
        "  run += 1\n",
        "  print(\"Run n° \", run)\n",
        "  train_sentences_words = list(pd.Series(sentences_words_rest)[train_index])\n",
        "  train_sentences_tags = list(pd.Series(sentences_tags_rest)[train_index])\n",
        "  test_sentences_words = list(pd.Series(sentences_words_rest)[test_index])\n",
        "  test_sentences_tags = list(pd.Series(sentences_tags_rest)[test_index])\n",
        "  \n",
        "  vocab = set(itertools.chain(*[[w for w in s] for s in sentences_words])) \n",
        "  tags = set(itertools.chain(*[[w for w in s] for s in train_sentences_tags]))\n",
        "  sentenecs_lens = map(len, train_sentences_words)\n",
        "  MAX_LEN = 75#max(sentenecs_lens)\n",
        "  VOCAB_SIZE = len(vocab)\n",
        "  words_tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters=[], oov_token='__UNKNOWN__')\n",
        "  words_tokenizer.fit_on_texts(map(lambda s: ' '.join(s), sentences_words))\n",
        "  word_index = words_tokenizer.word_index\n",
        "  word_index['__PADDING__'] = 0\n",
        "  index_word = {i:w for w, i in word_index.items()}\n",
        "  print('Unique tokens:', len(word_index))\n",
        "  #we define train and test sequences\n",
        "  train_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), train_sentences_words))\n",
        "  test_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), test_sentences_words))\n",
        "  train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LEN)\n",
        "  test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LEN)\n",
        "  tags_tokenizer = Tokenizer(num_words=len(tags), filters='', oov_token='__UNKNOWN__', lower=False)\n",
        "  tags_tokenizer.fit_on_texts(map(lambda s: ' '.join(s), train_sentences_tags))\n",
        "  tag_index = {\"O\":0, \"I-aspect\":1,\"B-aspect\":2}\n",
        "  index_tag = {i:w for w, i in tag_index.items()}\n",
        "\n",
        "  index_tag_wo_padding = dict(index_tag)\n",
        "  \n",
        "  print('Unique tags:', len(tag_index))\n",
        "  train_tags = []\n",
        "  test_tags = []\n",
        "  train_tags = convert_tags_to_id(train_sentences_tags)\n",
        "  test_tags = convert_tags_to_id(test_sentences_tags)\n",
        "  train_tags_padded = pad_sequences(train_tags, maxlen=MAX_LEN)\n",
        "  test_tags_padded = pad_sequences(test_tags, maxlen=MAX_LEN)\n",
        "  \n",
        "  train_tags_padded = np.expand_dims(train_tags_padded, -1)\n",
        "  test_tags_padded = np.expand_dims(test_tags_padded, -1)\n",
        "  \n",
        "  num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
        "  embedding_matrix = np.zeros((num_words, 300))\n",
        "  for word, i in word_index.items():\n",
        "    if i >= VOCAB_SIZE:\n",
        "      continue\n",
        "    embedding_vector = embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  print(num_words)\n",
        "  pretrained_embedding_layer = Embedding(num_words,\n",
        "                                300,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_LEN,\n",
        "                                trainable=False)\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "  embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
        "  x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                      recurrent_dropout=0.2, dropout=0.2))(embedded_sequences)\n",
        "  x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "  x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "  out = TimeDistributed(Dense(3, activation=\"softmax\"))(x)\n",
        "  model = Model(sequence_input, out)\n",
        "  model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit(train_sequences_padded[:((int)(len(train_sentences_words)*0.8))], train_tags_padded[:((int)(len(train_sentences_words)*0.8))],\n",
        "            batch_size=32,\n",
        "            epochs=10,\n",
        "            validation_data=(train_sequences_padded[-((int)(len(train_sentences_words)*0.2)):], train_tags_padded[-((int)(len(train_sentences_words)*0.2)):]))\n",
        "  lstm_predicted = model.predict(test_sequences_padded)\n",
        "  lstm_predicted_tags = []\n",
        "  index_tag_wo_padding = {0:\"O\",1:\"I-aspect\", 2:\"B-aspect\"}\n",
        "  for s, s_pred in zip(test_sentences_words, lstm_predicted):\n",
        "    tags = np.argmax(s_pred, axis=1)\n",
        "    tags = list(map(index_tag_wo_padding.get,tags))[-len(s):]\n",
        "    lstm_predicted_tags.append(tags)\n",
        "  print(classification_report(test_sentences_tags, lstm_predicted_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0UL5e_zXbQ",
        "colab_type": "text"
      },
      "source": [
        "# Cross Domain Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJHsDWTyzUfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We split all the datasets into training and test set\n",
        "train_sentences_words_rest, test_sentences_words_rest, train_sentences_tags_rest, test_sentences_tags_rest = train_test_split(sentences_words_rest, sentences_tags_rest, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_lap, test_sentences_words_lap, train_sentences_tags_lap, test_sentences_tags_lap = train_test_split(sentences_words_lap, sentences_tags_lap, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_hotels, test_sentences_words_hotels, train_sentences_tags_hotels, test_sentences_tags_hotels = train_test_split(sentences_words_hotels, sentences_tags_hotels, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_comp, test_sentences_words_comp, train_sentences_tags_comp, test_sentences_tags_comp = train_test_split(sentences_words_comp, sentences_tags_comp, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_speaker, test_sentences_words_speaker, train_sentences_tags_speaker, test_sentences_tags_speaker = train_test_split(sentences_words_speaker, sentences_tags_speaker, test_size=0.2, random_state=2018)\n",
        "train_sentences_words_router, test_sentences_words_router, train_sentences_tags_router, test_sentences_tags_router = train_test_split(sentences_words_router, sentences_tags_router, test_size=0.2, random_state=2018)\n",
        "\n",
        "#We concatenate the datasets of 5 over 6 domains\n",
        "train_sentences_words = train_sentences_words_lap+ train_sentences_words_router + train_sentences_words_hotels + train_sentences_words_comp+ train_sentences_words_rest\n",
        "train_sentences_tags = train_sentences_tags_lap+ train_sentences_tags_router + train_sentences_tags_hotels + train_sentences_tags_comp + train_sentences_tags_rest\n",
        "val_sentences_words = test_sentences_words_lap + test_sentences_words_router + test_sentences_words_hotels+test_sentences_words_comp + test_sentences_words_rest\n",
        "val_sentences_tags = test_sentences_tags_lap + test_sentences_tags_router + test_sentences_tags_hotels + test_sentences_tags_comp +test_sentences_tags_rest\n",
        "\n",
        "#The remaining dataset is used as test set\n",
        "test_sentences_words = train_sentences_words_router + test_sentences_words_router\n",
        "test_sentences_tags = train_sentences_tags_router + test_sentences_tags_router\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z1J0t4UzuLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a039182d-33fe-4db8-ecaa-efa2b7244567"
      },
      "source": [
        "#Also in this case we perform 5 iterations\n",
        "vocab = set(itertools.chain(*[[w for w in s] for s in sentences_words])) \n",
        "tags = set(itertools.chain(*[[w for w in s] for s in sentences_tags]))\n",
        "for j in range(5):\n",
        "  j += 1\n",
        "  print(\"Run n° \", j)\n",
        "\n",
        "  sentenecs_lens = map(len, train_sentences_words)\n",
        "  MAX_LEN = 75#max(sentenecs_lens)\n",
        "  VOCAB_SIZE = len(vocab)\n",
        "  words_tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters=[], oov_token='__UNKNOWN__')\n",
        "  words_tokenizer.fit_on_texts(map(lambda s: ' '.join(s), sentences_words))\n",
        "  word_index = words_tokenizer.word_index\n",
        "  word_index['__PADDING__'] = 0\n",
        "  index_word = {i:w for w, i in word_index.items()}\n",
        "  print('Unique tokens:', len(word_index))\n",
        "  #we define train and test sequences\n",
        "  train_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), train_sentences_words))\n",
        "  test_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), test_sentences_words))\n",
        "  val_sequences = words_tokenizer.texts_to_sequences(map(lambda s: ' '.join(s), val_sentences_words))\n",
        "  train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LEN)\n",
        "  test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LEN)\n",
        "  val_sequences_padded = pad_sequences(val_sequences, maxlen=MAX_LEN)\n",
        "  tags_tokenizer = Tokenizer(num_words=len(tags), filters='', oov_token='__UNKNOWN__', lower=False)\n",
        "  tags_tokenizer.fit_on_texts(map(lambda s: ' '.join(s), train_sentences_tags))\n",
        "  tag_index = {\"O\":0, \"I-aspect\":1,\"B-aspect\":2}\n",
        "  index_tag = {i:w for w, i in tag_index.items()}\n",
        "\n",
        "  train_tags = []\n",
        "  test_tags = []\n",
        "  train_tags = convert_tags_to_id(train_sentences_tags)\n",
        "  test_tags = convert_tags_to_id(test_sentences_tags)\n",
        "  val_tags = convert_tags_to_id(val_sentences_tags)\n",
        "  train_tags_padded = pad_sequences(train_tags, maxlen=MAX_LEN)\n",
        "  test_tags_padded = pad_sequences(test_tags, maxlen=MAX_LEN)\n",
        "  val_tags_padded = pad_sequences(val_tags, maxlen=MAX_LEN)\n",
        "  print(train_tags_padded[1])\n",
        "  print(train_sentences_tags[1])\n",
        "  train_tags_padded = np.expand_dims(train_tags_padded, -1)\n",
        "  test_tags_padded = np.expand_dims(test_tags_padded, -1)\n",
        "  val_tags_padded = np.expand_dims(val_tags_padded, -1)\n",
        "  num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
        "  embedding_matrix = np.zeros((num_words, 300))\n",
        "  for word, i in word_index.items():\n",
        "    if i >= VOCAB_SIZE:\n",
        "      continue\n",
        "    embedding_vector = embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  print(num_words)\n",
        "  pretrained_embedding_layer = Embedding(num_words,\n",
        "                                300,\n",
        "                                embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_LEN,\n",
        "                                trainable=False)\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "  embedded_sequences = pretrained_embedding_layer(sequence_input)\n",
        "  x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(embedded_sequences)\n",
        "  x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "  x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "  out = TimeDistributed(Dense(len(tag_index), activation=\"softmax\"))(x)\n",
        "  model = Model(sequence_input, out)\n",
        "  model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.fit(train_sequences_padded, train_tags_padded,\n",
        "            batch_size=32,\n",
        "            epochs=10,\n",
        "            validation_data=(val_sequences_padded, val_tags_padded))\n",
        "  lstm_predicted = model.predict(test_sequences_padded)\n",
        "  lstm_predicted_tags = []\n",
        "  index_tag_wo_padding = {0:\"O\",1:\"I-aspect\", 2:\"B-aspect\"}\n",
        "  for s, s_pred in zip(test_sentences_words, lstm_predicted):\n",
        "    tags = np.argmax(s_pred, axis=1)\n",
        "    tags = list(map(index_tag_wo_padding.get,tags))[-len(s):]\n",
        "    lstm_predicted_tags.append(tags)\n",
        "  print(classification_report(test_sentences_tags, lstm_predicted_tags))\n",
        "  print_f1_on_file(\"/content/drive/My Drive/Tesi_ABSA/router_glove_sampled_f1_\"+str(j+1)+\".txt\",test_sentences_tags,lstm_predicted_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 512/2299 [=====>........................] - ETA: 6:28 - loss: 0.0337 - accuracy: 0.9898"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8rj7Ai1z3XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}